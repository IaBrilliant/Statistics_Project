import matplotlib.pyplot as plt
import numpy as np 
import STOM_higgs_tools as sht
from statistics import mean
from scipy import stats 
import pickle 

def delete(x):              #Previous algorithm didn't work
    if x > 122 and x < 127: #You can play around with these values and you will realise that lambda is the highest with these params. 
      return False          #This is due to the fact that the gaussian peak is approx. in this range
    else: 
      return True 

def findA(lamb, bin_edges, bin_heights): 
    count =0
    dArea=0
    for b in bin_heights:
        dArea+=b*(bin_edges[count+1]-bin_edges[count]) #total area under histrogram
        count+=1
    return dArea*lamb/(lamb**2*(np.exp(-104.0/lamb) - np.exp(-155.0/lamb))) #You can find A by equating areas under exp. and histogram  
    
params = {'figure.figsize': [5, 5]} #so it looks pretty 
plt.rcParams.update(params)

vals = sht.generate_data()
bin_heights, bin_edges, patches = plt.hist(vals, range = [104, 155], bins = 30) #doing histogram for specified range
bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])
plt.errorbar(bin_centers, bin_heights, yerr = np.sqrt(bin_heights), ls = '', color = 'red', lw = 1, capsize = 2.5)
#Statistical uncertainty is yerr = sqrt(N) where N is the height of the bin - Possion error (Computer Session 2)
plt.ylim(0, 2000)
plt.xlim(104, 155)

vals_truncated = filter(delete, vals) # Filter is fast and efficient in deleting
lamb = mean(vals_truncated) #There is a pre-written func to calculate mean
a = findA(lamb,bin_edges, bin_heights) #finding normalisation factor
expectation = sht.get_B_expectation(bin_edges, a, lamb) 
plt.plot(bin_edges, expectation)
plt.show()

Chi_sq_b = sht.get_B_chi_trunc(vals, [104, 121, 129.5, 155], [10, 15], a, lamb) #New function for finding the value of chi_sq (see updated sht)
Chi_sq_s = sht.get_B_chi(vals, [104, 155], 30, a, lamb) 
print("Normalised Chi-Squared (backgroung) value is", Chi_sq_b) #By normalised I mean divided by N degrees of freedom
#You can see that Chi-sq is approximately equal to the number of degrees of freedom, hence it is a good fit.
print("Normalised Chi-Squared (+signal) value is", Chi_sq_s) #By normalised I mean divided by N degrees of freedom
print("P-value is:", 1 - stats.chi2.cdf(Chi_sq_s * 28, 28))
#Typical significane level is 5%, hence we reject H0


#Saving the Chi_set separately
'''
Chi_set = np.zeros(10000)
for i in range(0, 10000):
    s = sht.generate_background(10e5, 30)
    heights, edges, patches_ = plt.hist(s, range = [104, 155], bins = 30)
    Lambd = mean(s)
    A_c = findA(Lambd, edges, heights)
    Chi_set[i] = (sht.get_B_chi(s, [104, 155], 30, A_c, Lambd))*28
    

f = open('Chi_Set_background', 'wb')
pickle.dump(Chi_set, f)
f.close()
print('P0 saved')
exit(0) # kills the program
'''
# code that loads Chi_set_back from file
f = open('Chi_Set_background', 'rb')
Chi_set = pickle.load(f)
f.close()
chi_heights, chi_edges, patches_ = plt.hist(Chi_set, range = [0, 90], bins = 90) #We can clearly see our value to be next to the peak
plt.show()
